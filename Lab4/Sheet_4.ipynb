{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "MOitTdg3yP-M"
   },
   "source": [
    "# Important Information\n",
    "\n",
    "This file has two usages. \n",
    "1. This file can be displayed in Jupyter. You can read the task and insert your answers here. Start the notebook from an Anaconda prompt and change to the working directory containing the *.ipynb file.\n",
    "2. You can execute the code in Google Colab making use of Keras and Tensorflow. If you do not want to create a Google Account, you have to create a local environment for Keras and Tensorflow.\n",
    "\n",
    "For submission, upload your edited notebook together with all used images in a seperate folder (/images)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "-CXSNwExJb1C"
   },
   "source": [
    "Setup for this exercise sheet. Download data and define Tensorflow version.\n",
    "Execute code only if you setup your enviroment correctly or if you are inside a colab enviroment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 129
    },
    "colab_type": "code",
    "id": "Q7aTStJHJaay",
    "outputId": "7cd84744-a02f-4db8-ade2-21c81f6c4039",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Der Befehl \"git\" ist entweder falsch geschrieben oder\n",
      "konnte nicht gefunden werden.\n",
      "ERROR:root:Line magic function `%tensorflow_version` not found.\n"
     ]
    }
   ],
   "source": [
    "! git clone https://gitlab+deploy-token-26:XBza882znMmexaQSpjad@git.informatik.uni-kiel.de/las/nndl.git\n",
    "\n",
    "%tensorflow_version 2.x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 1 (Learning in neural networks)\n",
    "\n",
    "a) Explain the following terms related to neural networks as short and precise as possible. \n",
    "\n",
    "* Loss function\n",
    "* Stochastic gradient descent\n",
    "* Mini-batch \n",
    "* Regularization\n",
    "* Dropout\n",
    "* Batch normalization\n",
    "* Learning with momentum\n",
    "* Data augmentation\n",
    "* Unsupervised pre-training / supervised fine-tuning\n",
    "* Deep learning\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Answer: Write your answer here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "d8ahoTHxBW82"
   },
   "source": [
    "b) Name the most important output activation functions f(z), i.e., activation function of the output neuron(s), together with a corresponding suitable loss function L (in both cases, give the mathematical equation). Indicate whether such a perceptron is used for a classification or a regression task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Answer: Write your answer here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "dHDU9m2hChSn"
   },
   "source": [
    "# Exercise 2 (Multi-layer perceptron – regression problem)\n",
    "\n",
    "The goal of this exercise is to train a multi-layer perceptron to solve a high difficulty level nonlinear regression problem. The data has been generated using an exponential function with the following shape:\n",
    "\n",
    "![IMAGE: perceptron](images/Eckerle4Dataset.png)\n",
    "\n",
    "This graph corresponds to the values of a dataset that can be downloaded from the Statistical Reference Dataset of the Information Technology Laboratory of the United States on this link:\n",
    "http://www.itl.nist.gov/div898/strd/nls/data/eckerle4.shtml\n",
    "\n",
    "This dataset is provided in the file Eckerle4.csv. Note that this dataset is divided into a training and test corpus comprising 60% and 40% of the data samples, respectively. Moreover, the input and output values are normalized to the interval [0, 1]. Basic code to load the dataset and divide it into a training and test corpus, normalizing the data and to apply a multi-layer perceptron is provided in the Jupyter notebook.\n",
    "\n",
    "Choose a suitable network topology (number of hidden layers and hidden neurons, potentially include dropout, activation function of hidden layers) and use it for the multi-layer perceptron defined in the Jupyter notebook. Set further parameters (learning rate, loss function, optimizer, number of epochs, batch size; see the lines marked with *# FIX!!!* in the Jupyter notebook). Try to avoid underfitting and overfitting. Vary the network and parameter configuration in order to achieve a network performance as optimal as possible. For each network configuration, due to the random components in the experiment, perform (at least) 4 different training and evaluation runs and report the mean and standard deviation of the training and evaluation results. Report on your results and conclusions.\n",
    "\n",
    "(Source of exercise: http://gonzalopla.com/deep-learning-nonlinear-regression)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from os.path import join\n",
    "from tensorflow.keras.layers import Dense, Dropout, Activation\n",
    "from tensorflow.keras import Model, Input, Sequential\n",
    "from tensorflow.keras.optimizers import SGD, Adam, Adadelta, Adagrad, Nadam, RMSprop\n",
    "from tensorflow.keras.utils import normalize\n",
    "import pandas\n",
    "from sklearn import preprocessing\n",
    "from sklearn import model_selection\n",
    "import sys\n",
    "\n",
    "###--------\n",
    "# load data\n",
    "###--------\n",
    "\n",
    "# Imports csv into pandas DataFrame object.\n",
    "path_to_task = \"nndl/Lab4\"\n",
    "Eckerle4_df = pandas.read_csv(join(path_to_task,\"Eckerle4.csv\"), header=0)\n",
    " \n",
    "# Converts dataframes into numpy objects.\n",
    "Eckerle4_dataset = Eckerle4_df.values.astype(\"float32\")\n",
    "# Slicing all rows, second column...\n",
    "X = Eckerle4_dataset[:,1]\n",
    "# Slicing all rows, first column...\n",
    "y = Eckerle4_dataset[:,0]\n",
    " \n",
    "# plot data\n",
    "plt.plot(X,y, color='red')\n",
    "plt.legend(labels=[\"data\"], loc=\"upper right\")\n",
    "plt.title(\"data\")\n",
    "plt.show()\n",
    "\n",
    "###-----------\n",
    "# process data\n",
    "###-----------\n",
    "\n",
    "# Data Scaling from 0 to 1, X and y originally have very different scales.\n",
    "X_scaler = preprocessing.MinMaxScaler(feature_range=(0, 1))\n",
    "y_scaler = preprocessing.MinMaxScaler(feature_range=(0, 1))\n",
    "X_scaled = ( X_scaler.fit_transform(X.reshape(-1,1)))\n",
    "y_scaled = (y_scaler.fit_transform(y.reshape(-1,1)).reshape(-1) )\n",
    " \n",
    "# Preparing test and train data: 60% training, 40% testing.\n",
    "X_train, X_test, y_train, y_test = model_selection.train_test_split( X_scaled, y_scaled, test_size=0.40, random_state=3)\n",
    "\n",
    "\n",
    "###-----------\n",
    "# define model\n",
    "###-----------\n",
    "\n",
    "num_inputs = X_train.shape[1] # should be 1 in case of Eckerle4\n",
    "num_hidden = ... # for each hidden layer: number of hidden units in form of a python list   # FIX!!!\n",
    "num_outputs = 1 # predict single number in case of Eckerle4\n",
    "\n",
    "activation = '...' # activation of hidden layers   # FIX!!!\n",
    "dropout = ... # 0 if no dropout, else fraction of dropout units (e.g. 0.2)   # FIX!!!\n",
    "\n",
    "# Sequential network structure.\n",
    "model = Sequential()\n",
    "\n",
    "if len(num_hidden) == 0:\n",
    "  print(\"Error: Must at least have one hidden layer!\")\n",
    "  sys.exit()  \n",
    "\n",
    "# add first hidden layer connecting to input layer\n",
    "model.add(Dense(num_hidden[0], input_dim=num_inputs, activation=activation))\n",
    "\n",
    "if dropout:\n",
    "  # dropout of fraction dropout of the neurons and activation layer.\n",
    "  model.add(Dropout(dropout))\n",
    "#  model.add(Activation(\"linear\"))\n",
    "\n",
    "# potentially further hidden layers\n",
    "for i in range(1, len(num_hidden)):\n",
    "  # add hidden layer with len[i] neurons\n",
    "  model.add(Dense(num_hidden[i], activation=activation))\n",
    "#  model.add(Activation(\"linear\"))\n",
    "\n",
    "# output layer\n",
    "model.add(Dense(1))\n",
    "\n",
    "# show how the model looks\n",
    "model.summary()\n",
    "\n",
    "# compile model\n",
    "opt = ... # FIX!!!\n",
    "model.compile(loss='...', optimizer=opt, metrics=[\"...\"])# FIX!!!\n",
    "\n",
    "# Training model with train data. Fixed random seed:\n",
    "np.random.seed(3)\n",
    "num_epochs = ...   # FIX !!!\n",
    "batch_size = ... # FIX !!! \n",
    "history = model.fit(X_train, y_train, epochs=num_epochs, batch_size=batch_size, verbose=2)\n",
    "\n",
    "###-----------\n",
    "# plot results\n",
    "###-----------\n",
    "\n",
    "print(\"final (mse) training error: %f\" % history.history['loss'][num_epochs-1])\n",
    "\n",
    "plt.plot(history.history['loss'], color='red', label = 'training loss')\n",
    "plt.legend(labels=[\"loss\"], loc=\"upper right\")\n",
    "plt.title(\"training (mse) error\")\n",
    "plt.show()\n",
    "\n",
    "# Plot in blue color the predicted data and in green color the\n",
    "# actual data to verify visually the accuracy of the model.\n",
    "predicted = model.predict(X_test)\n",
    "plt.plot(y_scaler.inverse_transform(predicted.reshape(-1,1)), color=\"blue\")\n",
    "plt.plot(y_scaler.inverse_transform(y_test.reshape(-1,1)), color=\"green\")\n",
    "plt.legend(labels=[\"predicted\", \"target\"], loc=\"upper right\")\n",
    "plt.title(\"evaluation on test corpus\")\n",
    "plt.show()\n",
    "print(\"test error: %f\" % model.evaluate(X_test, y_test)[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Answer: Write your answer here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "V0dZkCh_Cm2n"
   },
   "source": [
    "# Exercise 3 (Parameters of a multi-layer perceptron – digit recognition)\n",
    "\n",
    "The "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Answer: Write your answer here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Exercise 4 (Vanishing gradient)\n",
    "\n",
    "The "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Answer: Write your answer here."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Lab3.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
