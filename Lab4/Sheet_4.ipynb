{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "MOitTdg3yP-M"
   },
   "source": [
    "# Important Information\n",
    "\n",
    "This file has two usages. \n",
    "1. This file can be displayed in Jupyter. You can read the task and insert your answers here. Start the notebook from an Anaconda prompt and change to the working directory containing the *.ipynb file.\n",
    "2. You can execute the code in Google Colab making use of Keras and Tensorflow. If you do not want to create a Google Account, you have to create a local environment for Keras and Tensorflow.\n",
    "\n",
    "For submission, upload your edited notebook together with all used images in a seperate folder (/images)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "-CXSNwExJb1C"
   },
   "source": [
    "Setup for this exercise sheet. Download data and define Tensorflow version.\n",
    "Execute code only if you setup your enviroment correctly or if you are inside a colab enviroment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 129
    },
    "colab_type": "code",
    "id": "Q7aTStJHJaay",
    "outputId": "7cd84744-a02f-4db8-ade2-21c81f6c4039",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Der Befehl \"git\" ist entweder falsch geschrieben oder\n",
      "konnte nicht gefunden werden.\n",
      "ERROR:root:Line magic function `%tensorflow_version` not found.\n"
     ]
    }
   ],
   "source": [
    "! git clone https://gitlab+deploy-token-26:XBza882znMmexaQSpjad@git.informatik.uni-kiel.de/las/nndl.git\n",
    "\n",
    "%tensorflow_version 2.x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 1 (Learning in neural networks)\n",
    "\n",
    "a) Explain the following terms related to neural networks as short and precise as possible. \n",
    "\n",
    "* Loss function\n",
    "* Stochastic gradient descent\n",
    "* Mini-batch \n",
    "* Regularization\n",
    "* Dropout\n",
    "* Batch normalization\n",
    "* Learning with momentum\n",
    "* Data augmentation\n",
    "* Unsupervised pre-training / supervised fine-tuning\n",
    "* Deep learning\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Answer: Write your answer here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "d8ahoTHxBW82"
   },
   "source": [
    "b) Name the most important output activation functions f(z), i.e., activation function of the output neuron(s), together with a corresponding suitable loss function L (in both cases, give the mathematical equation). Indicate whether such a perceptron is used for a classification or a regression task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Answer: Write your answer here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "dHDU9m2hChSn"
   },
   "source": [
    "# Exercise 2 (Multi-layer perceptron – regression problem)\n",
    "\n",
    "The goal of this exercise is to train a multi-layer perceptron to solve a high difficulty level nonlinear regression problem. The data has been generated using an exponential function with the following shape:\n",
    "\n",
    "![IMAGE: perceptron](images/Eckerle4Dataset.png)\n",
    "\n",
    "This graph corresponds to the values of a dataset that can be downloaded from the Statistical Reference Dataset of the Information Technology Laboratory of the United States on this link:\n",
    "http://www.itl.nist.gov/div898/strd/nls/data/eckerle4.shtml\n",
    "\n",
    "This dataset is provided in the file Eckerle4.csv. Note that this dataset is divided into a training and test corpus comprising 60% and 40% of the data samples, respectively. Moreover, the input and output values are normalized to the interval [0, 1]. Basic code to load the dataset and divide it into a training and test corpus, normalizing the data and to apply a multi-layer perceptron is provided in the Jupyter notebook.\n",
    "\n",
    "Choose a suitable network topology (number of hidden layers and hidden neurons, potentially include dropout, activation function of hidden layers) and use it for the multi-layer perceptron defined in the Jupyter notebook. Set further parameters (learning rate, loss function, optimizer, number of epochs, batch size; see the lines marked with *# FIX!!!* in the Jupyter notebook). Try to avoid underfitting and overfitting. Vary the network and parameter configuration in order to achieve a network performance as optimal as possible. For each network configuration, due to the random components in the experiment, perform (at least) 4 different training and evaluation runs and report the mean and standard deviation of the training and evaluation results. Report on your results and conclusions.\n",
    "\n",
    "(Source of exercise: http://gonzalopla.com/deep-learning-nonlinear-regression)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from os.path import join\n",
    "from tensorflow.keras.layers import Dense, Dropout, Activation\n",
    "from tensorflow.keras import Model, Input, Sequential\n",
    "from tensorflow.keras.optimizers import SGD, Adam, Adadelta, Adagrad, Nadam, RMSprop\n",
    "from tensorflow.keras.utils import normalize\n",
    "import pandas\n",
    "from sklearn import preprocessing\n",
    "from sklearn import model_selection\n",
    "import sys\n",
    "\n",
    "###--------\n",
    "# load data\n",
    "###--------\n",
    "\n",
    "# Imports csv into pandas DataFrame object.\n",
    "path_to_task = \"nndl/Lab4\"\n",
    "Eckerle4_df = pandas.read_csv(join(path_to_task,\"Eckerle4.csv\"), header=0)\n",
    " \n",
    "# Converts dataframes into numpy objects.\n",
    "Eckerle4_dataset = Eckerle4_df.values.astype(\"float32\")\n",
    "# Slicing all rows, second column...\n",
    "X = Eckerle4_dataset[:,1]\n",
    "# Slicing all rows, first column...\n",
    "y = Eckerle4_dataset[:,0]\n",
    " \n",
    "# plot data\n",
    "plt.plot(X,y, color='red')\n",
    "plt.legend(labels=[\"data\"], loc=\"upper right\")\n",
    "plt.title(\"data\")\n",
    "plt.show()\n",
    "\n",
    "###-----------\n",
    "# process data\n",
    "###-----------\n",
    "\n",
    "# Data Scaling from 0 to 1, X and y originally have very different scales.\n",
    "X_scaler = preprocessing.MinMaxScaler(feature_range=(0, 1))\n",
    "y_scaler = preprocessing.MinMaxScaler(feature_range=(0, 1))\n",
    "X_scaled = ( X_scaler.fit_transform(X.reshape(-1,1)))\n",
    "y_scaled = (y_scaler.fit_transform(y.reshape(-1,1)).reshape(-1) )\n",
    " \n",
    "# Preparing test and train data: 60% training, 40% testing.\n",
    "X_train, X_test, y_train, y_test = model_selection.train_test_split( X_scaled, y_scaled, test_size=0.40, random_state=3)\n",
    "\n",
    "\n",
    "###-----------\n",
    "# define model\n",
    "###-----------\n",
    "\n",
    "num_inputs = X_train.shape[1] # should be 1 in case of Eckerle4\n",
    "num_hidden = ... # for each hidden layer: number of hidden units in form of a python list   # FIX!!!\n",
    "num_outputs = 1 # predict single number in case of Eckerle4\n",
    "\n",
    "activation = '...' # activation of hidden layers   # FIX!!!\n",
    "dropout = ... # 0 if no dropout, else fraction of dropout units (e.g. 0.2)   # FIX!!!\n",
    "\n",
    "# Sequential network structure.\n",
    "model = Sequential()\n",
    "\n",
    "if len(num_hidden) == 0:\n",
    "  print(\"Error: Must at least have one hidden layer!\")\n",
    "  sys.exit()  \n",
    "\n",
    "# add first hidden layer connecting to input layer\n",
    "model.add(Dense(num_hidden[0], input_dim=num_inputs, activation=activation))\n",
    "\n",
    "if dropout:\n",
    "  # dropout of fraction dropout of the neurons and activation layer.\n",
    "  model.add(Dropout(dropout))\n",
    "#  model.add(Activation(\"linear\"))\n",
    "\n",
    "# potentially further hidden layers\n",
    "for i in range(1, len(num_hidden)):\n",
    "  # add hidden layer with len[i] neurons\n",
    "  model.add(Dense(num_hidden[i], activation=activation))\n",
    "#  model.add(Activation(\"linear\"))\n",
    "\n",
    "# output layer\n",
    "model.add(Dense(1))\n",
    "\n",
    "# show how the model looks\n",
    "model.summary()\n",
    "\n",
    "# compile model\n",
    "opt = ... # FIX!!!\n",
    "model.compile(loss='...', optimizer=opt, metrics=[\"...\"])# FIX!!!\n",
    "\n",
    "# Training model with train data. Fixed random seed:\n",
    "np.random.seed(3)\n",
    "num_epochs = ...   # FIX !!!\n",
    "batch_size = ... # FIX !!! \n",
    "history = model.fit(X_train, y_train, epochs=num_epochs, batch_size=batch_size, verbose=2)\n",
    "\n",
    "###-----------\n",
    "# plot results\n",
    "###-----------\n",
    "\n",
    "print(\"final (mse) training error: %f\" % history.history['loss'][num_epochs-1])\n",
    "\n",
    "plt.plot(history.history['loss'], color='red', label = 'training loss')\n",
    "plt.legend(labels=[\"loss\"], loc=\"upper right\")\n",
    "plt.title(\"training (mse) error\")\n",
    "plt.show()\n",
    "\n",
    "# Plot in blue color the predicted data and in green color the\n",
    "# actual data to verify visually the accuracy of the model.\n",
    "predicted = model.predict(X_test)\n",
    "plt.plot(y_scaler.inverse_transform(predicted.reshape(-1,1)), color=\"blue\")\n",
    "plt.plot(y_scaler.inverse_transform(y_test.reshape(-1,1)), color=\"green\")\n",
    "plt.legend(labels=[\"predicted\", \"target\"], loc=\"upper right\")\n",
    "plt.title(\"evaluation on test corpus\")\n",
    "plt.show()\n",
    "print(\"test error: %f\" % model.evaluate(X_test, y_test)[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Answer: Write your answer here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "V0dZkCh_Cm2n"
   },
   "source": [
    "# Exercise 3 (Parameters of a multi-layer perceptron – digit recognition)\n",
    "\n",
    "The "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from os.path import join\n",
    "from tensorflow.keras.layers import Dense, Dropout, Activation\n",
    "from tensorflow.keras import Model, Input, Sequential\n",
    "from tensorflow.keras.optimizers import SGD, Adam, Adadelta, Adagrad, Nadam, RMSprop\n",
    "from tensorflow.keras.utils import normalize\n",
    "import tensorflow.keras.datasets as tfds\n",
    "\n",
    "###--------\n",
    "# load data\n",
    "###--------\n",
    "\n",
    "(training_input, training_target), (test_input, test_target)  = tfds.mnist.load_data()\n",
    "\n",
    "# Reserve 10,000 samples for validation\n",
    "validation_input = training_input[-10000:]\n",
    "validation_target = training_target[-10000:]\n",
    "training_input = training_input[:-10000]\n",
    "training_target = training_target[:-10000]\n",
    "\n",
    "print(\"training input shape: %s, training target shape: %s\"  % (training_input.shape, training_target.shape))\n",
    "print(\"validation input shape: %s, validation target shape: %s\"  % (validation_input.shape, validation_target.shape))\n",
    "print(\"test input shape: %s, test target shape: %s\"  % (test_input.shape, test_target.shape))\n",
    "# range of input values: 0 ... 255\n",
    "print(\"\\n\")\n",
    "\n",
    "# plot some sample images\n",
    "num_examples = 1\n",
    "for s in range(num_examples):\n",
    "  print(\"Example image, true label: %d\" % training_target[s])\n",
    "  plt.imshow(training_input[s], vmin=0, vmax=255, cmap=plt.cm.gray)\n",
    "  plt.show()\n",
    "\n",
    "###-----------\n",
    "# process data\n",
    "###-----------\n",
    "\n",
    "# Note: shuffling is performed in fit method\n",
    "\n",
    "# scaling inputs from range 0 ... 255 to range [0,1] if desired\n",
    "scale_inputs = True # scale inputs to range [0,1]\n",
    "if scale_inputs:\n",
    "  training_input = training_input / 255\n",
    "  validation_input = validation_input / 255 \n",
    "  test_input = test_input / 255\n",
    "\n",
    "print(\"min. training data: %f\" % np.min(training_input))\n",
    "print(\"max. training data: %f\" % np.max(training_input))\n",
    "print(\"min. validation data: %f\" % np.min(validation_input))\n",
    "print(\"max. validation data: %f\" % np.max(validation_input))\n",
    "print(\"min. test data: %f\" % np.min(test_input))\n",
    "print(\"max. test data: %f\" % np.max(test_input))\n",
    "\n",
    "# flatten inputs to vectors\n",
    "training_input = training_input.reshape(training_input.shape[0], training_input.shape[1] * training_input.shape[2])\n",
    "validation_input = validation_input.reshape(validation_input.shape[0], validation_input.shape[1] * validation_input.shape[2])\n",
    "test_input = test_input.reshape(test_input.shape[0], test_input.shape[1] * test_input.shape[2])\n",
    "print(training_input.shape)\n",
    "print(validation_input.shape)\n",
    "print(test_input.shape)\n",
    "\n",
    "num_classes = 10 # 10 digits\n",
    "\n",
    "###-----------\n",
    "# define model\n",
    "###-----------\n",
    "\n",
    "histories = {}\n",
    "opt_learning_rate = {}\n",
    "final_training_loss = {}\n",
    "final_training_accuracy = {}\n",
    "final_validation_loss = {}\n",
    "final_validation_accuracy = {}\n",
    "final_test_loss = {}\n",
    "final_test_accuracy = {}\n",
    "\n",
    "configurations = [\n",
    "        # single hidden layer with 50 neurons\n",
    "        {'learningRates': [0.001, 0.01, 0.1],\n",
    "         'hiddenLayerSizes': [50],\n",
    "         'solver': 'SGD',\n",
    "         'activation':'relu'}, # activation of hidden layers\n",
    "         \n",
    "        # single hidden layer with 100 neurons\n",
    "        {'learningRates': [0.001, 0.01, 0.1],\n",
    "         'hiddenLayerSizes': [100],\n",
    "         'solver': 'SGD',\n",
    "         'activation':'relu'}, # activation of hidden layers\n",
    "\n",
    "        # single hidden layer with 200 neurons\n",
    "        {'learningRates': [0.001, 0.01, 0.1],\n",
    "         'hiddenLayerSizes': [200],\n",
    "         'solver': 'SGD',\n",
    "         'activation':'relu'}, # activation of hidden layers\n",
    "         \n",
    "        # two hidden layers with 100 neurons each\n",
    "        {'learningRates': [0.001, 0.01, 0.1],\n",
    "         'hiddenLayerSizes': [100, 100],\n",
    "         'solver': 'SGD',\n",
    "         'activation':'relu'}, # activation of hidden layers\n",
    "         \n",
    "         # three hidden layers with 100 neurons each\n",
    "        {'learningRates': [0.001, 0.01, 0.1], \n",
    "         'hiddenLayerSizes': [100, 100, 100],\n",
    "         'solver': 'SGD',\n",
    "         'activation':'relu'}, # activation of hidden layers\n",
    "         \n",
    "         # four hidden layers with 100 neurons each\n",
    "        {'learningRates': [0.001, 0.01, 0.1],\n",
    "         'hiddenLayerSizes': [100, 100, 100, 100],\n",
    "         'solver': 'SGD',\n",
    "         'activation':'relu'}, # activation of hidden layers\n",
    "\n",
    "        # single hidden layer with 100 neurons, Adam\n",
    "        {'learningRates': [0.001, 0.01, 0.1],\n",
    "         'hiddenLayerSizes': [100],\n",
    "         'solver': 'Adam',\n",
    "         'activation':'relu'}, # activation of hidden layers\n",
    "\n",
    "        # single hidden layer with 100 neurons, AdaGrad\n",
    "        {'learningRates': [0.001, 0.01, 0.1],\n",
    "         'hiddenLayerSizes': [100],\n",
    "         'solver': 'Adagrad',\n",
    "         'activation':'relu'}, # activation of hidden layers\n",
    "\n",
    "        # single hidden layer with 100 neurons, SGD, logistic\n",
    "        {'learningRates': [0.001, 0.01, 0.1],\n",
    "         'hiddenLayerSizes': [100],\n",
    "         'solver': 'SGD',\n",
    "         'activation':'logistic'}, # activation of hidden layers\n",
    "]\n",
    "\n",
    "numRepetitions = 4 # repetitions of experiment due to stochastic nature\n",
    "\n",
    "num_inputs = training_input.shape[1] \n",
    "num_outputs = num_classes \n",
    "dropout = 0 # 0 if no dropout, else fraction of dropout units (e.g. 0.2)   # FIX!!!\n",
    "\n",
    "idx_config = 0\n",
    "\n",
    "for config in configurations:\n",
    "  print(\"=======\")\n",
    "  print(\"Now running tests for config\", config)\n",
    "\n",
    "  learningRates = config['learningRates']\n",
    "  num_hidden = config['hiddenLayerSizes']\n",
    "  solver = config['solver']\n",
    "  activation = config['activation']\n",
    "\n",
    "  # Sequential network structure.\n",
    "  model = Sequential()\n",
    "\n",
    "  if len(num_hidden) == 0:\n",
    "    print(\"Error: Must at least have one hidden layer!\")\n",
    "    sys.exit()  \n",
    "\n",
    "  # add first hidden layer connecting to input layer\n",
    "  model.add(Dense(num_hidden[0], input_dim=num_inputs, activation=activation))\n",
    "\n",
    "  if dropout:\n",
    "    # dropout of fraction dropout of the neurons and activation layer.\n",
    "    model.add(Dropout(dropout))\n",
    "  #  model.add(Activation(\"linear\"))\n",
    "\n",
    "  # potentially further hidden layers\n",
    "  for i in range(1, len(num_hidden)):\n",
    "    # add hidden layer with len[i] neurons\n",
    "    model.add(Dense(num_hidden[i], activation=activation))\n",
    "  #  model.add(Activation(\"linear\"))\n",
    "\n",
    "  # output layer\n",
    "  model.add(Dense(units=num_outputs, name = \"output\"))\n",
    "\n",
    "  # print configuration\n",
    "  print(\"\\nModel configuration: \")\n",
    "  print(model.get_config())\n",
    "  print(\"\\n\")\n",
    "\n",
    "  # show how the model looks\n",
    "  model.summary()\n",
    "\n",
    "  optLearningRate = 0\n",
    "  optValidationAccuracy = 0\n",
    "\n",
    "  histories_lr = [] # remember history for each learning rate\n",
    "\n",
    "  for idx_lr in range(len(learningRates)):\n",
    "  \n",
    "    print(\"MODIFYING LEARNING RATE\")\n",
    "    learningRate = learningRates[idx_lr]\n",
    "    print(\"learning rate = %f\" % learningRate)\n",
    "\n",
    "    train_loss = np.zeros(numRepetitions)\n",
    "    train_acc = np.zeros(numRepetitions)\n",
    "    val_loss = np.zeros(numRepetitions)\n",
    "    val_acc = np.zeros(numRepetitions)\n",
    "    test_loss = np.zeros(numRepetitions)\n",
    "    test_acc = np.zeros(numRepetitions)\n",
    "\n",
    "    histories_rep = [] # (temporarily) remember history of each repetition\n",
    "    for idx_rep in range(numRepetitions):\n",
    "      print(\"\\nIteration %d...\" % idx_rep)  \n",
    "      \n",
    "      # compile model\n",
    "      if solver == 'SGD':\n",
    "        opt = SGD(learning_rate=learningRate) # SGD or Adam, Nadam, Adadelta, Adagrad, RMSProp, potentially setting more parameters\n",
    "      elif solver == 'Adam':\n",
    "        opt = Adam(learning_rate=learningRate)\n",
    "      elif solver == 'Nadam':\n",
    "        opt = Adam(learning_rate=learningRate)\n",
    "      elif solver == 'Adadelta':\n",
    "        opt = Adam(learning_rate=learningRate)\n",
    "      elif solver == 'Adagrad':\n",
    "        opt = Adam(learning_rate=learningRate)\n",
    "      elif solver == 'RMSprop':\n",
    "        opt = RMSprop(learning_rate=learningRate)\n",
    "      model.compile(optimizer=opt,loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),metrics=['sparse_categorical_accuracy'])\n",
    "\n",
    "      # Training model with train data. Fixed random seed:\n",
    "      num_epochs = 30 # FIX !!!\n",
    "      batch_size = 1 # FIX !!! \n",
    "      history = model.fit(training_input, training_target, epochs=num_epochs, batch_size=batch_size, shuffle=\"True\", verbose=2)\n",
    "      histories_rep.append(history) # remember all histories from all repetitions\n",
    "      train_loss[idx_rep] = history.history['loss'][num_epochs-1] \n",
    "      train_acc[idx_rep] = history.history['sparse_categorical_accuracy'][num_epochs-1]\n",
    "      val_loss[idx_rep] = model.evaluate(validation_input, validation_target)[0]\n",
    "      val_acc[idx_rep] = model.evaluate(validation_input, validation_target)[1]\n",
    "      test_loss[idx_rep] = model.evaluate(test_input, test_target)[0]\n",
    "      test_acc[idx_rep] = model.evaluate(test_input, test_target)[1]\n",
    "\n",
    "    # print results:\n",
    "    print(\"training loss (in brackets: mean +/- std):\")\n",
    "    for i in range(numRepetitions):\n",
    "        print(\"%f\" % train_loss[i])\n",
    "    print(\"(%f +/- %f)\\n\" % (np.mean(train_loss), np.std(train_loss, ddof=1)))\n",
    "\n",
    "    print(\"training accuracy (in brackets: mean +/- std):\")\n",
    "    for i in range(numRepetitions):\n",
    "        print(\"%f\" % train_acc[i])\n",
    "    print(\"(%f +/- %f)\\n\" % (np.mean(train_acc), np.std(train_acc, ddof=1)))\n",
    "\n",
    "    print(\"validation loss (in brackets: mean +/- std):\")\n",
    "    for i in range(numRepetitions):\n",
    "        print(\"%f\" % val_loss[i])\n",
    "    print(\"(%f +/- %f)\\n\" % (np.mean(val_loss), np.std(val_loss, ddof=1)))\n",
    "\n",
    "    print(\"validation accuracy (in brackets: mean +/- std):\")\n",
    "    for i in range(numRepetitions):\n",
    "        print(\"%f\" % val_acc[i])\n",
    "    print(\"(%f +/- %f)\\n\" % (np.mean(val_acc), np.std(val_acc, ddof=1)))\n",
    "\n",
    "    print(\"test loss (in brackets: mean +/- std):\")\n",
    "    for i in range(numRepetitions):\n",
    "        print(\"%f\" % test_loss[i])\n",
    "    print(\"(%f +/- %f)\\n\" % (np.mean(test_loss), np.std(test_loss, ddof=1)))\n",
    "\n",
    "    print(\"test accuracy (in brackets: mean +/- std):\")\n",
    "    for i in range(numRepetitions):\n",
    "        print(\"%f\" % test_acc[i])\n",
    "    print(\"(%f +/- %f)\\n\" % (np.mean(test_acc), np.std(test_acc, ddof=1)))\n",
    "\n",
    "    # remember history of best repetition (based on maximal validation accuracy)\n",
    "    idx_best_rep = np.argmax(val_acc)\n",
    "\n",
    "    # determine optimal learning rate (based on mean validation accuracy over repetitions)\n",
    "    if np.mean(val_acc) > optValidationAccuracy:\n",
    "        optValidationAccuracy = np.mean(val_acc)\n",
    "        opt_learning_rate[idx_config] = learningRate  \n",
    "        # remember history\n",
    "        histories[idx_config] = histories_rep[idx_best_rep]\n",
    "        # remember evaluation results\n",
    "        final_training_loss[idx_config] = train_loss[idx_best_rep]\n",
    "        final_training_accuracy[idx_config] = train_acc[idx_best_rep]\n",
    "        final_validation_loss[idx_config] = val_loss[idx_best_rep]\n",
    "        final_validation_accuracy[idx_config] = val_acc[idx_best_rep]\n",
    "        final_test_loss[idx_config] = test_loss[idx_best_rep]\n",
    "        final_test_accuracy[idx_config] = test_acc[idx_best_rep]   \n",
    "\n",
    "  print(\"optimal learning rate for this configuration: %f\" % opt_learning_rate[idx_config])\n",
    "\n",
    "  idx_config = idx_config + 1\n",
    "\n",
    "###-----------------------\n",
    "# print evaluation results\n",
    "###-----------------------\n",
    "\n",
    "for i in range(len(configurations)): \n",
    "  print(\"\\nconfiguration %s:\\n\" % configuration[i])\n",
    "  print(\"optimal learning rate: %f\" % opt_learning_rate[i])\n",
    "  print(\"final training loss: %f\" % final_training_loss[i])\n",
    "  print(\"final training accuracy: %f\" % final_training_accuracy[i])\n",
    "  print(\"final validation loss: %f\" % final_validation_loss[i])\n",
    "  print(\"final validation accuracy: %f\" % final_validation_accuracy[i])\n",
    "  print(\"final test loss: %f\" % final_test_loss[i])\n",
    "  print(\"final test accuracy: %f\" % final_test_accuracy[i])\n",
    "\n",
    "###-----------\n",
    "# plot results\n",
    "###-----------\n",
    " \n",
    "# plot setup\n",
    "num_rows = np.ceil(len(configurations)/2)\n",
    "fig, axes = plt.subplots(num_rows, 2, figsize=(15, 10))\n",
    "fig.tight_layout() # improve spacing between subplots, doesn't work\n",
    "plt.subplots_adjust(left=0.125, right=0.9, bottom=0.1, top=0.9, wspace=0.2, hspace=0.2) # doesn't work\n",
    "legend = []\n",
    "i = 0\n",
    "axes_indices = {}\n",
    "\n",
    "for i in range(num_rows):\n",
    "  axes_indices[2*i] = (i, 0)\n",
    "  axes_indices[2*i+1] = (i, 1)\n",
    "\n",
    "for i in range(len(configurations)):\n",
    "  # plot loss    \n",
    "  axes[axes_indices[i]].set_title('configuration ' + str(i))\n",
    "  if i == 8 or i == 9:  \n",
    "    axes[axes_indices[i]].set_xlabel('Epoch number')\n",
    "  axes[axes_indices[i]].set_ylim(0, 1)\n",
    "  axes[axes_indices[i]].plot(histories[name].history['categorical_crossentropy'], color = 'blue', \n",
    "              label = 'training loss')\n",
    "  axes[axes_indices[i]].plot(histories[name].history['categorical_accuracy'], color = 'red', \n",
    "              label = 'traning accuracy')\n",
    "  axes[axes_indices[i]].legend()\n",
    "\n",
    "  i = i + 1\n",
    "\n",
    "\n",
    "# show the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Answer: Write your answer here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Exercise 4 (Vanishing gradient)\n",
    "\n",
    "The "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Answer: Write your answer here."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Lab3.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
